{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91648393",
   "metadata": {},
   "source": [
    "# ML & DL 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58d464d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 구현 기술\n",
    "# 구분 시기 특징 내용 예시\n",
    "# 1세대\n",
    "# 1950~1980년대\n",
    "# 제어 프로그램\n",
    "# 기계 및 가전제품에 탐재된 단순 제어 프로그램\n",
    "# 자동 세탁기\n",
    "# 2세대 \n",
    "# 1990년대\n",
    "# 경로 및 DB 탐색\n",
    "# 대량 정보와 규칙기반 경로 탐색, DB검색 후 정답 파악, 전문가 시스템\n",
    "# 검색DB\n",
    "# 3세대\n",
    "# 2000년대\n",
    "# 머신러닝\n",
    "# 컴퓨터 스스로 규칙 및 지식 학습, 예측 방법 파악, 인공신경막?\n",
    "# 문자 및 패턴인식\n",
    "# 4세대 \n",
    "# 2010년대 ~ 현재\n",
    "# 딥러닝\n",
    "# 추상화된 특징 표현 등 고급 지식 학습, 데이터 변형 및 인사이트 파악, 깊은 인공신경막?\n",
    "# 영상 및 음성인식, 자연어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55b065e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 & 딥러닝 관계\n",
    "# AI  ML  DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05e0d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신 러닝\n",
    "# 인공지능의 한 분야\n",
    "# 패턴인식과 컴퓨터 학습 이론의 연구로부터 진화한 분야\n",
    "# 경험적 데이터 기반 학습하고 예측 수행으로 스스로의 성능을 향상시키는 시스템과 이를 위한 알고리즘을 연구하고 구축하는 기술\n",
    "# 입력 데이터를 기반으로 예측이나 결정을 이끌어내기 위해 특정한 모델 구축 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a654647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신 러닝\n",
    "# 알고리즘 특징\n",
    "# 지도학습\n",
    "# 문제 + 결과 데이터\n",
    "# 예측 결과 제공\n",
    "# 회귀, 분류, 랭킹\n",
    "# 강화학습\n",
    "# 문제 데이터만 제공\n",
    "# 결과, 결과 평가 후 보상\n",
    "# 알고리즘 트레이닝\n",
    "# 상 최대화/ 벌 최소화 방향\n",
    "# 비지도 학습\n",
    "# 문제 데이터만 제공\n",
    "# 분석 결과 제공\n",
    "# 군집화, 토픽 모델링, 밀도 추정, 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc53e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 \n",
    "# 기법 및 모델   특징\n",
    "# 분류(Classification)\n",
    "# 데이터의 특정 그룹 부여하여 구분\n",
    "# 회귀(Regression)\n",
    "# 수치 데이터에 사용되는 기법, 연속형 결과 예측\n",
    "# 의사결정 나무\n",
    "# 트리 구조 형태의 예측 모델을 사용하는 기법\n",
    "# 인공 신경망\n",
    "# 생물의 신경 네트워크 구조와 기능 모방 기법\n",
    "# 유전자 프로그래밍\n",
    "# 생물의 진화 알고리즘에 기반한 기법\n",
    "# 군집화\n",
    "# 관측된 예를 군집하여 부분집합으로 배분 기법\n",
    "# 몬테카를로 방법\n",
    "# 무작위 추출된 난수 통해 확률로 계산하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19a9df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝(Deep Learning)\n",
    "# 머신 러닝의 한 분야 => 신경망\n",
    "# 학습 데이터를 구분하는 층을 많이 만들어 그 정확도를 올리는 방법\n",
    "# 2016년 알파고에 적용\n",
    "# 영상 인식, 음성 인식, 자연 언어 처리 등 분야에서 우수한 성능 발휘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29c9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝\n",
    "# 알고리즘    특징\n",
    "# 퍼셉트론\n",
    "# 1957년 고안된 알고리즘\n",
    "# 딥러닝 신경망의 기원이 되는 알고리즘\n",
    "# 다수의 신호를 입력 받아서 하나의 신호 출력(0/1)\n",
    "# 뉴런 또는 노드로부터 신호 입력\n",
    "# 뉴런 또는 노드로 신호 출력\n",
    "# 인공신경망\n",
    "# 1980년대부터 활발히 연구\n",
    "# 뇌신경망의 패턴 인식 방식의 통계학적 학습 알고리즘\n",
    "# 선형 맞춤과 비선형 맞춤 반복 수행하여 최적화\n",
    "# 심층 신경망\n",
    "# 신경망이 다수의 층의 깊이로써 구성된 개념\n",
    "# 입력층과 출력층 사이 다수의 은닉층으로 구성된 ANN\n",
    "# 분류 및 수치 예측을 위한 것\n",
    "# 이미지 트레이닝, 문자인식 매우 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06a6c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML & DL용 python 필수 라이브러리\n",
    "# 라이브러리     언어 python   특징\n",
    "# Numpy\n",
    "# 파이썬에서 수치해석, 통계, 과학 계산용 모듈\n",
    "# Numeric라이블러리를 이어 SciPy Core로 다시 2005년에 Numpy로 개명\n",
    "# array자료 생성, 색인, 처리, 연산 수행\n",
    "# Scipy, Pandas, matplotlib 등 다른 Python 패키지와 함께 사용\n",
    "\n",
    "# SciPy\n",
    "# 과학 계산용 함수를 모아 놓은 오픈 소스 파이썬 패키지\n",
    "# 선형 대수, 함수 최적화, 신호 처리, 특수 수학 함수, 통계 분포 등\n",
    "\n",
    "# Matplotlib\n",
    "# Matlab을 기반으로 파이썬으로 확장한 패키지로 그래프 패키지\n",
    "# 데이터와 분석 결과를 다양한 관점으로 시각화 가능\n",
    "\n",
    "# Pandas\n",
    "# Series와 DataFrame 데이터 구조 제공 및 데이터 처리 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fb3edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML 라이브러리 프레임 워크\n",
    "# 라이브러리 특징\n",
    "# Scikit-learn\n",
    "# 다양한 지도학습, 비지도학습 알고리즘 구현\n",
    "# python기반으로 Numpy, SciPy이용한 고속화 지원\n",
    "\n",
    "# Statsmodels\n",
    "# 검정 및 추정, 회귀분석, 시계열분석 등 다양한 통계분석\n",
    "# 회귀분석의 경우 patsy 패키지 포함\n",
    "\n",
    "# NLTK\n",
    "# Natural Language Toolkit 약자 자연어 처리 라이브러리\n",
    "\n",
    "# MLib\n",
    "# 아파치 스마크용 머신러닝 라이브러리\n",
    "\n",
    "# Weka \n",
    "# 자바로 쓰인 데이터마이닝 라이브러리 / GUI 제공\n",
    "\n",
    "# OpenCV\n",
    "# 2000년에 공개되었으며 이미지 데이터 처리 함수 풍부\n",
    "\n",
    "# Pytorch\n",
    "# 페이스북 인공지능 연구팀에서 개발\n",
    "\n",
    "# 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79950ebd",
   "metadata": {},
   "source": [
    "## Maching Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa5cbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝\n",
    "# 개발자가 작성한 프로그램에 따라 동작하는 sw가 아닌 많은 데이터 기반 학습 통해 답을 출력할 수 있는 방법 터득\n",
    "\n",
    "# [스팸필터 기능]\n",
    "# 스팸의 기준 다양                        ->             자동 스팸 구분\n",
    "# 다양한 형태 스팸 데이터\n",
    "\n",
    "# [집값 예상]\n",
    "# 주거지역 정보                          ->              주소로 집값 예측\n",
    "# 집값 정보 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e81a89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 과정\n",
    "# 목표 설정\n",
    "# 구현 및 개발 목표 설정\n",
    "\n",
    "# 데이터 준비\n",
    "# 데이터 수집\n",
    "# 데이터 추출\n",
    "# 데이터 가공\n",
    "\n",
    "# 데이터학습\n",
    "# 학습 방법 선택\n",
    "# 매개변수 조정\n",
    "# 학습으로 모델 구축\n",
    "\n",
    "# 모델평가\n",
    "#                  모바일 웹 응용sw\n",
    "# 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef72d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 방법\n",
    "# 학습 알고리즘                 특징\n",
    "# 지도학습\n",
    "# 문제 + 결과 데이터 모두 제공\n",
    "# 예측 결과 제공\n",
    "# 회귀, 분류, 랭킹\n",
    "\n",
    "# 강화 학습\n",
    "# 문제 데이터만 제공\n",
    "# 결과, 결과 평가 후 보상\n",
    "# 알고리즘 트레이닝(상 최대화/ 벌 최소화 방향)\n",
    "\n",
    "# 비지도 학습\n",
    "# 문제 데이터만 제공\n",
    "# 분석 결과 제공\n",
    "# 군집화, 토픽 모델링, 밀도 추정, 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36f3c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신 러닝 학습 단계\n",
    "# 데이터 분포\n",
    "# 분석\n",
    "# 학습기법설정\n",
    "# 학습\n",
    "# 가설\n",
    "# 데이터적용           가설변경\n",
    "# 오차계산\n",
    "# 검증 \n",
    "# 검출기생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3e9b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋(dataset)\n",
    "# 정확한 규칙/패턴을 찾기 위해 필요한 데이터 집합\n",
    "# 종류\n",
    "# 규칙/패턴을 찾기 위한 데이터 => 학습용 데이터셋\n",
    "# 규칙/패턴을 찾는 과정 중 검사 데이터 => 검증용 데이터셋\n",
    "# 규칙/패턴을 평가 위한 데이터 => 테스트용 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d123b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류\n",
    "# 규칙/패턴 찾기 위해 입력되는 데이터\n",
    "# 입력/문제/피쳐(특성)\n",
    "\n",
    "# 입력 데이터에 해당하는 정답데이터 (지도 학습 시 제공)\n",
    "# 타겟/정답/클래스/라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3f054f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 학습 준비\n",
    "# 데이터셋(dataset) => 데이터+라벨\n",
    "#                     데이터로만 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e0e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도/교사학습(supervised learning)\n",
    "# 가장 널리 사용되는 방법\n",
    "# 구성: 입력(문제)데이터 + 타겟(정답)데이터\n",
    "# 일반적으로 우리가 학교에서 배우는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e86b0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 종류\n",
    "# 회귀(Regression)\n",
    "# 시간:점수와 관계\n",
    "# 공부 시간에 따른 점수 예측\n",
    "\n",
    "# 분류(Classification)\n",
    "# 이진분류(Binary Classification)\n",
    "# 시간:합격여부 관계\n",
    "# 공부 시간에 따른 합격 / 예측\n",
    "\n",
    "# 다중분류(Multilabel Classification)\n",
    "# 시간:학점 관계\n",
    "# 공부 시간에 따른 학점(A~F) 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b3a18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도학습(supervised learning)\n",
    "# 회귀\n",
    "# 공부시간과 점수\n",
    "# 5시간 공부하면 몇점?\n",
    "# 공부시간과 점수 사이의 일정한 규칙(패턴) 찾기\n",
    "# 규칙(패턴)  시간*10+4 = 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1211c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진분류\n",
    "# 공부시간과 합격여부\n",
    "# 5시간 공부하면 합격? 불합격?\n",
    "# 2021년 고3수험생의 1일 평균 공부시간과 합격/불합격 데이터 수집 후 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c458c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중분류\n",
    "# 공부시간과 학점\n",
    "# 5시간 공부하면 학점은?\n",
    "# A+, A, B+, B, ..., F\n",
    "# 10가지 학점 중 하나의 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ba97e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도학습\n",
    "# Regression(회귀)\n",
    "# 데이터를 기반으로 새로운 데이터에 대한 결과 예측하는 기술\n",
    "# 데이터와 데이터 분석 기반 가설 필요\n",
    "# 해당 가설에 오차를 줄여 예측 적중률을 높이는 것이 목표\n",
    "\n",
    "# 구현 기법\n",
    "# Linear Regression\n",
    "# Logistic Regression\n",
    "# Multi-variable Linear Regression & Matrix\n",
    "# 그 외 다양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e7e67890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification(분류)\n",
    "# 데이터 분석 결과 결과(라벨)로 데이터를 나누는 경우\n",
    "# 두가지 결과로 나누는 경우 => 이진 분류\n",
    "# 세가지 이상 결과로 나누는 경우 => 다중 분류\n",
    "\n",
    "# 구현 기법\n",
    "# KNN(K-nearest neighbor)\n",
    "# Native Bayes\n",
    "# SVM(Support Vector Machine)\n",
    "# Decision Tree\n",
    "# Random Forest\n",
    "# 그 외 다양 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942d990",
   "metadata": {},
   "source": [
    "## Scikit-learn 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4efbd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "# 머신러닝 알고리즘, 전처리, 검증 등 기능 제공 라이브러리\n",
    "# Numpy 기반 속도 최적화\n",
    "# 바로 테스트 가능한 다양한 샘플 데이터 제공\n",
    "# 많이 사용되는 다른 라이브러리와 호환 가능\n",
    "# BSD 라이선스로 무료.상업적 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9862459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "# 분류   모듈명          내장 기능\n",
    "# 예제 데이터 \n",
    "# sklearn.datasets\n",
    "# 연습용 데이터셋\n",
    "\n",
    "# 피처 처리\n",
    "# sklearn.preprocessing \n",
    "# 전처리 관련 기법 (원핫 인코딩, 정규화, 스케일링 등)\n",
    "\n",
    "# sklearn.feature_selection\n",
    "# 모델에 중요한 영향을 미치는 피처를 탐색 및 선택하는 기법\n",
    "\n",
    "# sklearn.feature_extraction\n",
    "# 원시 데이터로부터 피처를 추출하는 기능\n",
    "# 이미지에 대한 피처 추출은 하위 모듈 image에, 텍스트 데이터의 피처 추출은 하위 모듈 text에 지원 API가 있음\n",
    "\n",
    "# 차원 축소\n",
    "# sklearn.decomposition\n",
    "# 차원 축소 관련 알고리즘 계열 (PCA, NMF, Truncated SVD 등)\n",
    "\n",
    "# 기계학습 알고리즘\n",
    "# sklearn.ensemble\n",
    "# 앙상블 알고리즘 계열(랜덤 포레스트, 에이다 부스트, 배깅 등)\n",
    "\n",
    "# sklearn.linear_model\n",
    "# 선형 알고리즘 계열 (선형회귀, 로지스틱 회귀, SGD 등)\n",
    "\n",
    "# sklearn.naive_bayes\n",
    "# 나이브 베이즈 알고리즘 계열 (베르누이 NB, 가우시안 NB, 다항분포 NB 등)\n",
    "\n",
    "# sklearn.neighbors \n",
    "# 최근접 이웃 알고리즘 계열 (K-NN 등)\n",
    "\n",
    "# sklearn.SVM\n",
    "# Support Vector Machine 계열 알고리즘\n",
    "\n",
    "# sklearn.tree\n",
    "# 의사 결정 나무 계열 알고리즘\n",
    "\n",
    "# sklearn.cluster\n",
    "# 비지도 학습(클러스터링) 알고리즘 (KMeans, DBSCAN 등)\n",
    "\n",
    "# 검증, 하이퍼 파라미터 튜닝, 데이터 분리\n",
    "# sklearn.model_Selection\n",
    "# 검증, 하이퍼 파라미터 데이터 분리 등\n",
    "# (cross_validate, GridSearchCV, train_test_split, learning_curve 등)\n",
    "\n",
    "# 모델 평가\n",
    "# sklearn.metrics\n",
    "# 모델의 성능을 측정 및 평가하는 기법\n",
    "# (accuray, precision, recall, ROC curve 등)\n",
    "\n",
    "# 유틸리티\n",
    "# sklearn.pipeline\n",
    "# 피처 처리 등의 변환과 기계학습 알고리즘 등을 연쇄적으로 실행하는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2055a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit_learn.metrics 모듈\n",
    "# scikit-learn 에서 모델 및 성능평가에 사용되는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "90f8b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn.metrics 모듈\n",
    "# 함수                역할\n",
    "# accuracy_score()\n",
    "# 정확도 측정\n",
    "\n",
    "# confusion_matrix()\n",
    "# Classification에서 실제와 결과를 메트릭스로 표현\n",
    "# 성능지표(confusion matrix) 출력, 정확도 평가\n",
    "\n",
    "# precision_score()\n",
    "# 정밀도 측정\n",
    "\n",
    "# recall_score\n",
    "# 재현율 측정\n",
    "\n",
    "# f1_score()\n",
    "# 정밀도와 재현율의 평균\n",
    "\n",
    "# classfication_report()\n",
    "# 분류 학습 후 결과 보고서를 출력\n",
    "# precision, recall, f1-score, support 항목 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab73e1",
   "metadata": {},
   "source": [
    "## REGRESSION(회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "343c284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 지도학습의 학습 방법 중 하나\n",
    "# 선형과 비선형\n",
    "# 입력데이터에 대한 타겟 값(수치)를 예측하는 것\n",
    "# 키에 따른 몸무게, 피자 도착 시간, 경계 성장률 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1527ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression 회귀\n",
    "# 구현 단계\n",
    "# 1. 데이터를 기반으로 데이터의 관계 분석\n",
    "# 2. 데이터 분석 결과 수학적 계산 가능 공식 가설 설정\n",
    "# 3. 데이터의 가설 공식에 적용 학습\n",
    "# 4. 공식에 사용되는 계수 찾기\n",
    "# 5. 테스트 데이터 활용 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa1c9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 데이터 & 타겟\n",
    "# 분석\n",
    "# x와 y 데이터 관계 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d43677b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 Regression\n",
    "# linear regression(선형 회귀)\n",
    "# 선형 형태 데이터 분포\n",
    "# 많은 데이터 만족하는 직선\n",
    "# 1차 함수 y=Wx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e45aff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 regression\n",
    "# multiple regression(다중 회귀)\n",
    "# 선형 형태 데이터 분포\n",
    "# 많은 데이터 만족하는 직선\n",
    "# 1차 함수 y=ax1+bx2+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cbfa84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비선형 regression\n",
    "# polynomial regression (다항 회귀)\n",
    "# 곡선 형태 데이터 분포\n",
    "# 많은 데이터 만족하는 곡선\n",
    "# 2차 함수 y=a2x^2 + a1x^1 + c\n",
    "# N차 함수 y=adx^d + ... +a2x^2+a1x^1+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "edd25b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비선형 regression\n",
    "# logistiv regression(로지스틱 회귀) <- 신경만 DL\n",
    "# s자 형태 데이터 분포\n",
    "# 많은 데이터 만족하는 s형 곡선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1eeaf2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 직선 y=ax+b\n",
    "# y=ax1+a2x2+b\n",
    "# 곡선 y=adx^d+...+a2x^2+a1x^1+b\n",
    "# y=타겟, x=특성\n",
    "# 모델 학습 통해 최적의 a,b 찾기\n",
    "# a,b는 모델 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95abef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 모델 성능 평가\n",
    "# 실제값과 모델 예측값과의 차이 <- 오차\n",
    "# 오차 = 0 >>> 모델 100%: 불가능\n",
    "# 오차 최소화되도록 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c18e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 오차 함수(cost/loss function)\n",
    "# 오차\n",
    "# T-y=T-(Wx+b)\n",
    "# H(x)=1x+0 가설"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ccd5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 오차 함수 (cost/loss function)\n",
    "# 단순오차함수(RSS:residual sum of squares)\n",
    "# 예측값과 타겟값과의 차이를 제곱해서 모두 더한 함수\n",
    "\n",
    "# 평균제곱오차 함수(MSE:mean squared error)\n",
    "# 예측값과 타겟값과의 차이를 제곱해서 모두 더한 후 평균 취하는 함수\n",
    "# 이상치 값이 존재하면 수치가 많이 커짐(민감함)\n",
    "\n",
    "# 평균절대값오차함수(MAE:mean absoulte error)\n",
    "# 예측값과 타겟값과의 차이를 절대값을 모두 더한 후 평균 취하는 함수\n",
    "# 변동성이 큰 지표와 작은 지표 함께 비교에 좋음\n",
    "\n",
    "# 평균절대값오차함수(RMAE:Root Mean Squared Error)\n",
    "# MSE에 루트를 취한 값\n",
    "# 에러에 따른 손실이 기하 급수적으로 올라가는 상황에서 쓰기 적합\n",
    "# MAE와 함께 가장 일반적으로 많이 쓰이는 회귀모델 성능분석지표\n",
    "\n",
    "# R2 Score (Coefficient of Detremination)\n",
    "# 주어진 데이터들에 기반해 만들어진 모델이 주어진 데이터들을 얼마나 잘 '설명하는가'에 대한 수치  => 결정계수\n",
    "# 예측 모델과 실제 모델이 얼마나 강한 상관관계(correlated) 가지는가\n",
    "# 총 제곱합: 관측값과 평균의 차이 기반\n",
    "# 회귀제곱합: 회귀 예측값과 평균의 차이 기반\n",
    "# 1-((y_true - y_pred)**2).sum() / ((y_true-y_true.mean())**2).sum()\n",
    "# 값이 1에 가까울수록 성능이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b42f3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# a, b 업데이트 최적화 기법\n",
    "# 손실 함수의 값이 최소화가 될 수 있도록 a, b 값 계속 업데이트 필요\n",
    "# 처음 랜덤한 값으로 초기화 후 적정한 값으로 계속 업데이트 진행\n",
    "# 최적의 값 찾아내기\n",
    "\n",
    "# 대표적인 최적화 기법\n",
    "# 경사하강법 (gradient descent)\n",
    "# 아담스최적화 기법\n",
    "# RMshop 기법 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c17ae750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 경사하강법(gradient descent)\n",
    "# 정답이 주어진 데이터가 있을 경우 최적의 선형회귀모델 직선을 긋고\n",
    "# 그 직선의 MSE 즉 cost function을 최소로 만드는 직선을 찾아야 함\n",
    "# cost를 최소로 하는 직선을 구하는 과정 => 학습\n",
    "# 학습에 사용되는 알고리즘이 바로 '경사하강법'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ff63261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "# 학습속도: learning rate\n",
    "# 경사 하강법에서 학습 단계별로 움직이는 학습 속도 정의\n",
    "# W값을 조정해 가면서 cost값이 최소가 되는 값을 찾기 위한 것\n",
    "# W값이 다음 W값이 되는 속도\n",
    "\n",
    "# 오버슈팅(Over shooting)\n",
    "# 학습속도가 큰 경우 발생\n",
    "# 최소값으로 내려가지 않고 반대편으로 넘어가 무한대\n",
    "\n",
    "# 스몰 러닝 레이트(small learning rate)\n",
    "# 학습속도가 매우 작은 경우 발생\n",
    "# 0.0001\n",
    "# 최소값 가기 전에 학습 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddbac9",
   "metadata": {},
   "source": [
    "## 비선형 REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5f1286ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "# 입력 데이터와 타겟값의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 회귀 분석과 목표 동일\n",
    "# linear regression과 달리 타켓값이 범주형 데이터\n",
    "# 입력 데이터에 대한 결과가 특정 분류로 나뉘어 짐\n",
    "# 기본 규제: L2 적용\n",
    "# 비선형적 데이터에도 적용 가능  => 커널 로지스틱 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fce55948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# 종류\n",
    "# 이진분류: 2가지(0과 1) 분류\n",
    "# 다중분류: 3가지 이상으로 분류 \n",
    "\n",
    "# 각 범주로 분류될 확률로 결과 반환\n",
    "# 모든 범주의 분류 확률 합 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "495f5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 - 공부 시간과 성적  y = ax + b\n",
    "# 예시 - 공부 시간과 합격 여부 sigmoid(y = ax + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a27b5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# 시그모이드(sigmoid) 함수: s자형 곡선 또는 시그모이드 곡선을 갖는 수학 함수\n",
    "# 출력 범위 [0~1]\n",
    "# 양성 클래스 1, yes, 합격  0.5 <= y=ax+b\n",
    "# 음성 클래스 0, no, 불합격  0.5 > y=ax+b\n",
    "# 입력 범위 (-inf, inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e105735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistric regression\n",
    "# s자 형태 데이터 분포\n",
    "# 많은 데이터 만족 s형 곡선\n",
    "# z=ax+b\n",
    "# sigmoid(z)\n",
    "# p=1/1+e^(-z)\n",
    "# p: 확률값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73558169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀 적용 손실함수\n",
    "# 선형회귀 cost function을 로지스틱회귀 가설 적용하면 w의 값에 대한 cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fb2a5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀 적용 손실함수\n",
    "# 시작점\n",
    "# 경사하강법으로 구해지는 지점\n",
    "# 원래의 최소 기울기 지점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "81372a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# 실제값과 오차 관계\n",
    "# a가 클수록 경사 커짐\n",
    "# a가 작을수록 경사 작아짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "db2f02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제값과 오차 관계\n",
    "# 파랑색 선 실제 값 1\n",
    "# -ylog(H(x))\n",
    "\n",
    "# 빨강색 선 실제 값 0\n",
    "# (1-y)log(1-H(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0d031be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절편과 오차 관계\n",
    "# b가 클수록 오른쪽 이동\n",
    "# b가 작을수록 왼쪽 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "edb1a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 로그함수\n",
    "# cost(H(x),y)=-ylog(H(X))-(1-y)log(1-H(x))\n",
    "\n",
    "# 손실(오차) = -평균(ylog(H(x))) + (1-y)log(1-H(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "effbfe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# 시그모이드(sigmoid) 함수\n",
    "# numpy 모듈\n",
    "# numpy.exp(1개)  확률값\n",
    "\n",
    "# scipy 모듈\n",
    "# scipy.special.expit(1개)  확률값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b78f575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스(softmax) 함수\n",
    "# 다중 분류 시에 각 라벨(클래스/타겟)에 대한 확률 추정 함수\n",
    "# 시그모이드 함수에서 유래\n",
    "# 입력값: 여러개\n",
    "# 출력값: 0~1 사이의 실수 '확률'로 해석 가능\n",
    "# 출력 총합: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ee9746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스(softmax) 함수\n",
    "# 다중 입력 softmax 다중 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac8b7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy 모듈\n",
    "# scipy.special.softmax(여러개)  확률값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7ad69e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit_learn lib 사용 - 학습객체\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# (penalty = L2  규제 사용 기준 지정 (L2 = MSE + 가중치 제곱합)\n",
    "# dual  이중 또는 초기 공식\n",
    "# tol  정밀도\n",
    "# C=1.0  규제 강도 (cost function), 큰값(약)-작은값(강)\n",
    "# fit_intercept=True  절편 존재 여부 설정\n",
    "# intercept_scaling=1  정규화 효과 정도\n",
    "# class_weight = 1 클래스 가중치\n",
    "# random_state  난수 seed 설정\n",
    "# solver='lbfgs'  최적화문제 사용 알고리즘\n",
    "# max_iter  계산 작업 수\n",
    "# multi_class  다중 분류 시 (ovr, multinomial, auto)로 설정\n",
    "# verbose  동작 과정에 대한 출력 메시지\n",
    "# warm_start  이전 모델을 초기화로 적합하게 사용할 것인지 여부\n",
    "# n_jobs  병렬 처리할 때 사용되는 CPU 코어 수\n",
    "# l1_ratio  L1 규제의 비율(Elastic-Net에만 사용))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aded4",
   "metadata": {},
   "source": [
    "## ML OPTIMIZATION & MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c08345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화\n",
    "# 목적함수(Objective Function를 최대한, 혹은 최소화하는 파라미터 조합을 찾는 과정)\n",
    "# 경사하강법 (Gradient Decent): 비용(cost)/손실함수(loss), 오차(error)\n",
    "# 경사상승법(Gradient Acscent): 이익(profit), 점수(score)\n",
    "\n",
    "# 모델 평가 시 손실/ 비용 함수 값이 최소가 될 때 최적의 모델\n",
    "# 손실/ 비용 함수 값이 최소가 되는 모델 파라미터를 찾는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a2fafcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화\n",
    "# ML/DL 오차와 기울기 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f434b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법(Gradient Descent)\n",
    "# 등산 후 하산 해야 하는 상황\n",
    "# 현재 내 위치로 부터 경사가 가장 가파른 방향으로 이동하는 것\n",
    "# 최소점을 만족하는 파라미터 값을 찾는 것\n",
    "# 일차 미분 이용한 최적화 기법\n",
    "# 미분 통해 gradient를 구한 후 반대 방향(음수)으로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c4874e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법\n",
    "# 학습속도(learning rate)\n",
    "# 경사 하강법에서 학습 단계별로 움직이는 학습 속도 정의\n",
    "# w값 조정해 가면서 cost 값이 최소가 되는 값을 찾기 위한 것\n",
    "# w값이 다음 w값이 되는 속도\n",
    "\n",
    "# 오버슈팅(over shooting)\n",
    "# 학습속도가 큰 경우 발생\n",
    "# 최소값으로 내려가지 않고 반대편으로 넘어가 무한대\n",
    "\n",
    "# 스몰 러닝 레이트(small learning rate)\n",
    "# 학습속도가 매우 작은 경우 발생\n",
    "# 예) 0.00001\n",
    "# 최소값 가기 전에 학습 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fcbb4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법(gradient descent)\n",
    "# 에포크(epoch) <- scikit-learn에서 max_iter 파라미터\n",
    "# 전체 샘플을 모두 사용하는 한 번 반복 의미\n",
    "\n",
    "# 배치 사이즈 (batch_siae)\n",
    "# 전체 샘플을 쪼개어 나눈 크기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "852a26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법\n",
    "# 배치 학습/ 오프라인 학습\n",
    "# 모든 데이터를 한꺼번에 학습\n",
    "# 시간과 자원이 많이 소모되어 오프라인에서 수행\n",
    "# 새로운 데이터 학습 위해 전체 데이터를 처음부터 다시 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bf10d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점진적 학습 / 온라인 학습\n",
    "# 데이터를 순차적으로 한 개씩 또는 작은 묶음으로 학습 진행\n",
    "# 연속적으로 데이터를 받고 변화에 빠르게 적용 가능\n",
    "# 기존 모델에 새로운 데이터 추가 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc03a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법\n",
    "# 배치경사하강법 BGD\n",
    "# 전체 학습 데이터 gradient 계산\n",
    "# 많은 시간 및 계산량 소요되지만 정확도 높음\n",
    "\n",
    "# 확률적경사하강법 SGE\n",
    "# 한 번에 한 개 데이터만 랜덤 샘플링 통해 추출 후 gradient 계산\n",
    "# BGD에 비해 다소 부정확할 수 있지만 속도가 훨씬 빠름\n",
    "\n",
    "# 미니배치경사하강법 MSGD\n",
    "# BGD와 SGD 를 절충하여 일부 학습 데이터를 gradient 계산\n",
    "# SGD에 비해 정확도 높고 BGD에 비해 효율적\n",
    "\n",
    "# Momentum\n",
    "# 기울기 방향으로 힘을 받아 물체가 가속되어 공이 구르는 듯한 움직임\n",
    "# 이전 값과 비교 후 같은 방향으로 업데이트 진행 -> 경사하강 + 관성, SGD 보다 빠름\n",
    "\n",
    "# AdaGrad\n",
    "# 변수들을 update할 때 각각의 변수마다 step size를 다르게 설정해서 이동하는 방식\n",
    "# 기존 기울기 값을 제곱한 값을 더하여 학습률을 조정\n",
    "\n",
    "# RMSProp / AdaDelta\n",
    "# AdaGrad의 갱신 속도 느려지는 단점을 해결한 방법\n",
    "# 먼 과거의 기울기는 조금 반영하고 최신의 기울기를 많이 반영\n",
    "\n",
    "# Adam\n",
    "# 과거 미분값 계속 가중평균 내면서 효율적 업데이트\n",
    "# AdaGrd + Momentum 방식 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a631f25",
   "metadata": {},
   "source": [
    "## SGD OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2f113276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 최적화\n",
    "# 확률적 경사 하강법을 이용하여 구현하는 모델들\n",
    "# 분류와 회귀 모두에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ce436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 최적화\n",
    "# SGDClassifier\n",
    "# 이진분류: 로지스틱회귀, 이진 크로스에느로피 손실함수\n",
    "# 다중분류: 크로스엔트로피 손실함수\n",
    "\n",
    "# SGDRegressor\n",
    "# 평균제곱근오차 손실함수 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
