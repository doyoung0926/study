{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754af1c0",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "---\n",
    "- 패키지 설치\n",
    "    * NLTK : pip install nltk\n",
    "    * KoNLPy : pip install Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f09f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지 설치\n",
    "!pip install nltk   # 한글이 안 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "673601d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy   # 한글 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b43e6",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    "- 문장/문서를 의미를 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함\n",
    "- 종류\n",
    "    * 문장 토큰화\n",
    "    * 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941b83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2941da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ff602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Corpus 말뭉치 데이터셋 다운로드 받기\n",
    "# nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc974dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1=\"when tokenizing a Unicode string.\\\n",
    "           NLTK tokenizers can produce token-spans.\\\n",
    "           when tokenizing a Unicode string.\"\n",
    "raw_text2=\"This particular tokenizer requires the Punkt sentence tokenization.\\\n",
    "           We can also operate at the level of sentences.\\\n",
    "           There are numerous ways to tokenize text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "419f591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1=word_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47eaca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'tokenizing', 'a', 'Unicode', 'string', '.', 'NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.', 'when', 'tokenizing', 'a', 'Unicode', 'string', '.']\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ca82397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_text=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5bffed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when tokenizing a Unicode string.           NLTK tokenizers can produce token-spans.           when tokenizing a Unicode string.',\n",
       " 'This particular tokenizer requires the Punkt sentence tokenization.           We can also operate at the level of sentences.           There are numerous ways to tokenize text.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daf4ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=sent_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314e5048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when tokenizing a Unicode string.', 'NLTK tokenizers can produce token-spans.', 'when tokenizing a Unicode string.'] 3\n"
     ]
    }
   ],
   "source": [
    "print(result, len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8e8ea",
   "metadata": {},
   "source": [
    "### 여러 문장에 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a264f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => ['when tokenizing a Unicode string.', 'NLTK tokenizers can produce token-spans.', 'when tokenizing a Unicode string.']\n",
      "ele => when tokenizing a Unicode string.\n",
      "wordResult => ['when', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "ele => NLTK tokenizers can produce token-spans.\n",
      "wordResult => ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.']\n",
      "ele => when tokenizing a Unicode string.\n",
      "wordResult => ['when', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "sent => ['This particular tokenizer requires the Punkt sentence tokenization.', 'We can also operate at the level of sentences.', 'There are numerous ways to tokenize text.']\n",
      "ele => This particular tokenizer requires the Punkt sentence tokenization.\n",
      "wordResult => ['This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', '.']\n",
      "ele => We can also operate at the level of sentences.\n",
      "wordResult => ['We', 'can', 'also', 'operate', 'at', 'the', 'level', 'of', 'sentences', '.']\n",
      "ele => There are numerous ways to tokenize text.\n",
      "wordResult => ['There', 'are', 'numerous', 'ways', 'to', 'tokenize', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "for sent in raw_text:\n",
    "    total_token=[]\n",
    "    # 문장 추출\n",
    "    sentResult=sent_tokenize(sent)\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f'sent => {sentResult}')\n",
    "    \n",
    "    for ele in sentResult:\n",
    "        print(f'ele => {ele}')\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(f'wordResult => {wordResult}')\n",
    "        total_token.append(wordResult)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd351f78",
   "metadata": {},
   "source": [
    "### 한글\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1c8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분리 객체\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e907beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '월요일', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리\n",
    "result=okt.morphs(\"오늘은 월요일입니다.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a7fa83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리 후 태깅(Tagging) => 품사\n",
    "result2=okt.pos(\"오늘은 월요일입니다.\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52668612",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=okt.pos(\"오늘은 월요일입니다.\", stem=True)  # stem 어간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "549a3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35258c4d",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화\n",
    "    * 대문자 또는 소문자로 통일\n",
    "    * 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4c0b8",
   "metadata": {},
   "source": [
    "### [2-1] 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94c3692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "752f447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9129f577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48a069",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dedaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6857011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "lstem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a497ad9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work', 'work')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('working'), lstem.stem('worked'), lstem.stem('worken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c91a4b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c863b350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deb38b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebfc5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "529dcbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('working', 'v'), wlemma.lemmatize('worked', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "862e2544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('amusing', 'v'), wlemma.lemmatize('amused', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bd57c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb2aa6e6",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    "- 텍스트 => 수치화\n",
    "- 희소벡터(OHE) : BOW 방식 --> Count 기반, TF-IDF 기반\n",
    "- 밀집벡터 : Embedding 방식, Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8eeb5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a147725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de30f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c084aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48efeed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ohe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce6ce5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t2\n",
      "  (0, 22)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 27)\t2\n",
      "  (0, 28)\t2\n",
      "  (0, 31)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 19)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 30)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ret), ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26f21fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ret.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6b12177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n",
      "[[0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 2 0 0 0 0 0 1 0 0 0 1 2 2 0 0 2]\n",
      " [1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 2 1 1 1 0 1 1 1 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(ret.shape, ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "965c7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF 기반\n",
    "tfIdf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c31128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfIdf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26193e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e73e65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7da7479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.15342554 0.         0.21563424\n",
      "  0.         0.         0.         0.         0.21563424 0.\n",
      "  0.         0.         0.         0.21563424 0.43126847 0.\n",
      "  0.         0.         0.         0.         0.21563424 0.\n",
      "  0.         0.         0.21563424 0.43126847 0.43126847 0.\n",
      "  0.         0.43126847]\n",
      " [0.19800527 0.19800527 0.19800527 0.14088238 0.19800527 0.\n",
      "  0.19800527 0.19800527 0.19800527 0.19800527 0.         0.19800527\n",
      "  0.19800527 0.19800527 0.19800527 0.         0.         0.19800527\n",
      "  0.39601054 0.19800527 0.19800527 0.19800527 0.         0.19800527\n",
      "  0.19800527 0.19800527 0.         0.         0.         0.19800527\n",
      "  0.19800527 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a53fbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977ec76",
   "metadata": {},
   "source": [
    "# Tokenizer 객체 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79adac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c096cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2f674e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90c37e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11acdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ad1cba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myToken.texts_to_sequences(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4300bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken.fit_on_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1238494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c8007c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'wiki': 2, 'is': 3, 'in': 4, 'the': 5, 'that': 6, 'to': 7, 'web': 8, 'a': 9, 'of': 10, 'users': 11, 'content': 12, 'allows': 13, 'create': 14, 'edit': 15, 'page': 16, 'any': 17, 'has': 18, 'simple': 19, 'pages': 20, 'on': 21, 'it': 22, 'ward': 23, 'original': 24, 'description': 25, 'simplest': 26, 'online': 27, 'database': 28, 'could': 29, 'possibly': 30, 'work': 31, 'piece': 32, 'server': 33, 'software': 34, 'freely': 35, 'using': 36, 'browser': 37, 'supports': 38, 'hyperlinks': 39, 'text': 40, 'syntax': 41, 'for': 42, 'creating': 43, 'new': 44, 'crosslinks': 45, 'between': 46, 'internal': 47, 'fly': 48, 'unusual': 49, 'among': 50, 'group': 51, 'communication': 52, 'mechanisms': 53, 'organization': 54, 'contributions': 55, 'be': 56, 'edited': 57, 'addition': 58, 'itself': 59, 'like': 60, 'many': 61, 'concepts': 62, 'open': 63, 'editing': 64, 'some': 65, 'profound': 66, 'subtle': 67, 'effects': 68, 'usage': 69, 'allowing': 70, 'everyday': 71, 'site': 72, 'exciting': 73, 'encourages': 74, 'democratic': 75, 'use': 76, 'promotes': 77, 'composition': 78, 'by': 79, 'nontechnical': 80}\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42a9bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('wiki', 5), ('is', 5), ('in', 5), ('ward', 1), ('original', 1), ('description', 1), ('the', 5), ('simplest', 1), ('online', 1), ('database', 1), ('that', 4), ('could', 1), ('possibly', 1), ('work', 1), ('a', 3), ('piece', 1), ('of', 3), ('server', 1), ('software', 1), ('allows', 2), ('users', 3), ('to', 4), ('freely', 1), ('create', 2), ('and', 6), ('edit', 2), ('web', 4), ('page', 2), ('content', 3), ('using', 1), ('any', 2), ('browser', 1), ('supports', 1), ('hyperlinks', 1), ('has', 2), ('simple', 2), ('text', 1), ('syntax', 1), ('for', 1), ('creating', 1), ('new', 1), ('pages', 2), ('crosslinks', 1), ('between', 1), ('internal', 1), ('on', 2), ('fly', 1), ('unusual', 1), ('among', 1), ('group', 1), ('communication', 1), ('mechanisms', 1), ('it', 2), ('organization', 1), ('contributions', 1), ('be', 1), ('edited', 1), ('addition', 1), ('itself', 1), ('like', 1), ('many', 1), ('concepts', 1), ('open', 1), ('editing', 1), ('some', 1), ('profound', 1), ('subtle', 1), ('effects', 1), ('usage', 1), ('allowing', 1), ('everyday', 1), ('site', 1), ('exciting', 1), ('encourages', 1), ('democratic', 1), ('use', 1), ('promotes', 1), ('composition', 1), ('by', 1), ('nontechnical', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "945c5705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9], [], []]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myToken.texts_to_sequences('and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38552dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606475cc",
   "metadata": {},
   "source": [
    "### Tokenizer 객체 -----------------------------------------\n",
    "- 제공한 문서/문장에 대한 단어사전(voca)\n",
    "- 단어사전(voca)에 존재하지 않는 단어 => Out Of Voca : oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a24f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',  \n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "697374dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=1)   # oov_token=0 안 됨!\n",
    "\n",
    "# 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad4e3bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# 단어 인덱스 : 단어 인덱스\n",
    "print(tokenizer.word_index)   # 사전?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "024ed2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('i', 2), ('love', 3), ('my', 4), ('dog', 3), ('cat', 1), ('you', 2), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 단어 출력 개수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "46fbc8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화\n",
    "print(tokenizer.texts_to_sequences(['We think my dog is amazing?']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3cffa",
   "metadata": {},
   "source": [
    "### 패딩(Padding)\n",
    "---\n",
    "- 길이가 모두 다른 문장들을 동일 길이로 맞추기 위한 과정\n",
    "- 길이 기준 설정\n",
    "- 긴 경우 => 앞/뒤 중 선택\n",
    "- 짧은 경우 => 앞/뒤 중 선택\n",
    "- 값 => 패딩에 들어갈 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c86864de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9dbd3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6cf66193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23734678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  5,  3,  2,  4],\n",
       "       [ 0,  0,  0,  5,  3,  2,  7],\n",
       "       [ 0,  0,  0,  6,  3,  2,  4],\n",
       "       [ 8,  6,  9,  2,  4, 10, 11]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding=pad_sequences(result)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06007a9c",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding 변환\n",
    "---\n",
    "- sklearn OneHotEncoder 객체 생성\n",
    "- keras to_categorical 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1a3c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c80289f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_voca : 4\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화\n",
    "seq_voca=tokenizer.texts_to_sequences(sentences)\n",
    "print(f'seq_voca : {len(seq_voca)}')\n",
    "print(seq_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b05a758a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(seq_voca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "62fe778b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7a5f9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7564e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49c6e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab0deb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  5,  3,  2,  4],\n",
       "       [ 0,  0,  0,  5,  3,  2,  7],\n",
       "       [ 0,  0,  0,  6,  3,  2,  4],\n",
       "       [ 8,  6,  9,  2,  4, 10, 11]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ae4c94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  2,  4,  0,  0,  0],\n",
       "       [ 5,  3,  2,  7,  0,  0,  0],\n",
       "       [ 6,  3,  2,  4,  0,  0,  0],\n",
       "       [ 8,  6,  9,  2,  4, 10, 11]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c4a77c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  2,  4,  0],\n",
       "       [ 5,  3,  2,  7,  0],\n",
       "       [ 6,  3,  2,  4,  0],\n",
       "       [ 9,  2,  4, 10, 11]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e7e4f",
   "metadata": {},
   "source": [
    "## 실습 -------------------------------------------\n",
    "---\n",
    "- 단어 단위 토큰화\n",
    "- 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd29c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 추출\n",
    "from nltk import corpus\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a48cca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d649b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ac43bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens=word_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30b18bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, list)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordTokens), type(wordTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b4d34a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens2 : 85\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "wordTokens2=[]\n",
    "for word in wordTokens:\n",
    "    if word not in en_stopwords:\n",
    "        wordTokens2.append(word)\n",
    "        \n",
    "print(f'wordTokens2 : {len(wordTokens2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28d05fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens3 : 85\n"
     ]
    }
   ],
   "source": [
    "wordTokens3=[ word for word in wordTokens if word not in en_stopwords ]\n",
    "\n",
    "print(f'wordTokens3 : {len(wordTokens3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa87dcb",
   "metadata": {},
   "source": [
    "## FILE 읽고 벡터화\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b1ac6",
   "metadata": {},
   "source": [
    "### [1] 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4fc982e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE='./data/example.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1c87b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE, mode='r') as f:\n",
    "    fileData=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "191fb984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(fileData), type(fileData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6f70e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy\\n\\n\\nHenry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution. The school is sponsored by the Ford Motor Company, Wayne County Regional Educational Service Agency and The Henry Ford Museum and admits high school students. It is located in Dearborn, Michigan on the campus of the Henry Ford museum. Enrollment is taken from a lottery in the area and totaled 467 in 2010.[1]\\nFreshman meet inside the main museum building in glass walled classrooms, while older students use a converted carousel building and Pullman cars on a siding of the Greenfield Village railroad. Classes are expected to include use of the museum artifacts, a tradition of the original Village Schools. When the Museum was established in 1929, it included a school which served grades kindergarten to college/trade school ages. The last part of the original school closed in 1969.\\nThe Henry Ford Learning Institute is using the Henry Ford Academy model for further charter schools including the Power House High in Chicago and Alameda School for Art + Design in San Antonio.\\nThe building received the international annual design award of the Council of Educational Facilities Planners International for 2001, the James D. MacConnell Award for outstanding new educational facilities. Notable attendees include Chris Stroud and Isaac Sudut.\\nSee also[edit]'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a1bf6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 => 문자열 리스트\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "data_list=sent_tokenize(fileData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4dc92392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list => 12\n"
     ]
    }
   ],
   "source": [
    "print(f'data_list => {len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5f66a354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy\\n\\n\\nHenry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution.',\n",
       " 'The school is sponsored by the Ford Motor Company, Wayne County Regional Educational Service Agency and The Henry Ford Museum and admits high school students.',\n",
       " 'It is located in Dearborn, Michigan on the campus of the Henry Ford museum.',\n",
       " 'Enrollment is taken from a lottery in the area and totaled 467 in 2010.',\n",
       " '[1]\\nFreshman meet inside the main museum building in glass walled classrooms, while older students use a converted carousel building and Pullman cars on a siding of the Greenfield Village railroad.',\n",
       " 'Classes are expected to include use of the museum artifacts, a tradition of the original Village Schools.',\n",
       " 'When the Museum was established in 1929, it included a school which served grades kindergarten to college/trade school ages.',\n",
       " 'The last part of the original school closed in 1969.',\n",
       " 'The Henry Ford Learning Institute is using the Henry Ford Academy model for further charter schools including the Power House High in Chicago and Alameda School for Art + Design in San Antonio.',\n",
       " 'The building received the international annual design award of the Council of Educational Facilities Planners International for 2001, the James D. MacConnell Award for outstanding new educational facilities.',\n",
       " 'Notable attendees include Chris Stroud and Isaac Sudut.',\n",
       " 'See also[edit]']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076c557",
   "metadata": {},
   "source": [
    "### [2] 토큰화 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e55f3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToken=Tokenizer()\n",
    "# raw_data용 단어사전 생성\n",
    "fileToken.fit_on_texts(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "334f2134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index : 137개\n",
      "{'the': 1, 'in': 2, 'ford': 3, 'of': 4, 'henry': 5, 'school': 6, 'a': 7, 'and': 8, 'museum': 9, 'for': 10, 'is': 11, 'building': 12, 'academy': 13, 'to': 14, 'educational': 15, 'main': 16, 'classrooms': 17, 'charter': 18, 'by': 19, 'high': 20, 'students': 21, 'it': 22, 'on': 23, 'use': 24, 'village': 25, 'include': 26, 'original': 27, 'schools': 28, 'design': 29, 'international': 30, 'award': 31, 'facilities': 32, 'houses': 33, 'some': 34, 'first': 35, 'united': 36, 'states': 37, 'be': 38, 'developed': 39, 'jointly': 40, 'global': 41, 'corporation': 42, 'public': 43, 'education': 44, 'major': 45, 'nonprofit': 46, 'cultural': 47, 'institution': 48, 'sponsored': 49, 'motor': 50, 'company': 51, 'wayne': 52, 'county': 53, 'regional': 54, 'service': 55, 'agency': 56, 'admits': 57, 'located': 58, 'dearborn': 59, 'michigan': 60, 'campus': 61, 'enrollment': 62, 'taken': 63, 'from': 64, 'lottery': 65, 'area': 66, 'totaled': 67, '467': 68, '2010': 69, '1': 70, 'freshman': 71, 'meet': 72, 'inside': 73, 'glass': 74, 'walled': 75, 'while': 76, 'older': 77, 'converted': 78, 'carousel': 79, 'pullman': 80, 'cars': 81, 'siding': 82, 'greenfield': 83, 'railroad': 84, 'classes': 85, 'are': 86, 'expected': 87, 'artifacts': 88, 'tradition': 89, 'when': 90, 'was': 91, 'established': 92, '1929': 93, 'included': 94, 'which': 95, 'served': 96, 'grades': 97, 'kindergarten': 98, 'college': 99, 'trade': 100, 'ages': 101, 'last': 102, 'part': 103, 'closed': 104, '1969': 105, 'learning': 106, 'institute': 107, 'using': 108, 'model': 109, 'further': 110, 'including': 111, 'power': 112, 'house': 113, 'chicago': 114, 'alameda': 115, 'art': 116, 'san': 117, 'antonio': 118, 'received': 119, 'annual': 120, 'council': 121, 'planners': 122, '2001': 123, 'james': 124, 'd': 125, 'macconnell': 126, 'outstanding': 127, 'new': 128, 'notable': 129, 'attendees': 130, 'chris': 131, 'stroud': 132, 'isaac': 133, 'sudut': 134, 'see': 135, 'also': 136, 'edit': 137}\n"
     ]
    }
   ],
   "source": [
    "print(f'word_index : { len(fileToken.word_index) }개\\n{ fileToken.word_index }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54e485",
   "metadata": {},
   "source": [
    "### [3] 문장 수치화 & 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f4d4a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqData=fileToken.texts_to_sequences(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d94b2d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 5, 3, 9, 12, 33, 34, 4, 1, 17, 10, 1, 5, 3, 13, 5, 3, 13, 11, 1, 35, 18, 6, 2, 1, 36, 37, 14, 38, 39, 40, 19, 7, 41, 42, 43, 44, 8, 7, 45, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "print(seqData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f9da5f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy\n",
      "\n",
      "\n",
      "Henry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution.\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e6baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
